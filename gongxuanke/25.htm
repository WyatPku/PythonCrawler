<html lang="zh"><head>
<title>课程详细信息</title>
<meta charset="utf-8"><link href="style.css" rel="stylesheet" type="text/css">
</head>
<body style="padding:0px 0px 0px 0px;margin:0px 0px 0px 0px;font:微软雅黑 ,tahoma,arial,verdana,sans-serif;">
	<table align="center" width="1000" border="0" cellspacing="3px">
	<tbody><tr>
			<td></td>
	</tr>
	
	   
	   
		<tr>
			<td>
			<p style="color: #39961D;font-size: 16px;font-weight: bold;text-align:center">机器学习前沿：在线学习和优化课程详细信息</p>
			</td>
		</tr>
		<tr>
			<td>
			<table class="pkuportal-table-portlet" width="100%">
				<tbody><tr>
					<th class="pkuportal-td-title" width="25%">课程号</th>
					<td width="25%"><span>04833340</span></td>
					<th class="pkuportal-td-title" width="25%">学分</th>
					<td width="25%"><span>2</span></td>
				</tr>
				<tr>
					<th class="pkuportal-td-title">英文名称</th>
					<td colspan="3"><span>Advanced Machine Learning: Online Learning and Optimization</span></td>
				</tr>
				<tr>
					<th class="pkuportal-td-title" width="10%">先修课程</th>
					<td colspan="9"><span>无</span></td>
				</tr>
				<tr>
					<th class="pkuportal-td-title" width="10%">中文简介</th>
					<td colspan="9"><span>设计依照环境自适应变化的自动系统，一直是计算机科学和工程希望能够实现的一个重要目标。在某些情形下，外在环境往往过于复杂，以致不能很好直接建模，这个时候最好的方法是采用一种鲁邦的方式：通过和环境的交互，来持续优化系统。作为机器学习的一个子领域，在线学习对此类问题提供了坚实的理论基础。这门课程将会主要讲授在线学习，包括基本的技巧和想法。我们同时会讨论它和机器学习其它领域的联系，应用，以及如何用此类技巧来设计高效大规模优化方法。</span></td>
				</tr>
				<tr>
					<th class="pkuportal-td-title" width="10%">英文简介</th>
					<td colspan="9"><span>Designing autonomous systems that can adapt to their environments is arguably one of the most important goals in computer science and engineering. In some cases the environment is too complex to be modeled, and the best is to take a robust approach: continuously optimize the system as it interacts with its environment. Online learning, a subfield of machine learning, provides the theoretical foundations to solve such problems. The course will provide an introduction to online learning, covering the basic techniques and ideas. We will also discuss its connections and applications to other areas of machine learning, as well as how the same techniques lead to efficient methods for large scale optimization.</span></td>
				</tr>
				<tr>
					<th class="pkuportal-td-title" width="10%">开课院系</th>
					<td colspan="9"><span>信息科学技术学院</span></td>
				</tr>
				<tr>
					<th class="pkuportal-td-title" width="10%">通选课领域</th>
					<td colspan="9"><span>&nbsp;</span></td>
				</tr>
				<tr>
					<th class="pkuportal-td-title" width="10%">是否属于艺术与美育</th>
					<td colspan="9"><span>否</span></td>
				</tr>
				<tr>
					<th class="pkuportal-td-title" width="10%">平台课性质</th>
					<td colspan="9"><span>&nbsp;</span></td>
				</tr>
				<tr>
					<th class="pkuportal-td-title" width="10%">平台课类型</th>
					<td colspan="9"><span>&nbsp;</span></td>
				</tr>
				<tr>
					<th class="pkuportal-td-title" width="10%">授课语言</th>
					<td colspan="9"><span>英文</span></td>
				</tr>
				<tr>
					<th class="pkuportal-td-title" width="10%">教材</th>
					<td colspan="9"><span>Introduction to Online Convex Optimization,E. Hazan,2016；
<br>Online learning with predictable sequences,A. Rakhlin and K. Sridharan,Relax and randomize: from value to algorithms,A. Rakhlin, O. Shamir, and K. Sridharan,Prediction, Learning, and Games,N. Cesa-Bianchi and G. Lugosi,Cambridge University Press,2006,Online learning and online convex optimization,S. Shalev-Shwartz,2012,Adaptive subgradient methods for online learning and stochastic optimization,John Duchi, Elad Hazan, and Yoram Singer,Achieving all with no parameters: AdaNormalHedge,H. Luo and R. E. Schapire,A unified modular analysis of online and stochastic optimization: Adaptivity, optimism, non-convexity,P. Joulani, A. Gyorgy, and Cs. Szepesvari,MetaGrad: Multiple learning rates in online learning,T. van Erven, W. M. Koolen,Online learning,A. Gyorgy, D. Pal, Cs. Szepesvari,Coin Betting and Parameter-Free Online Learning,F. Orabona and D. Pal,Regret analysis of stochastic and nonstochastic multi-armed bandit problems,S. Bubeck and N. Cesa-Bianchi,A Proximal Stochastic Gradient Method with Progressive Variance Reduction,L. Xiao and T. Zhang,(Bandit) convex optimization with biased noisy gradient oracles,X. Hu, L. A. Prashanth, A. Gyorgy, and Cs. Szepesvari,On the complexity of best arm identification in multi-armed bandit models,E. Kaufmann, O. Cappe, and A. Garivier,Bandit based Monte-Carlo planning,L. Kocsis L, Cs. Szepesvari,</span></td>
				</tr>
				<tr>
					<th class="pkuportal-td-title" width="10%">参考书</th>
					<td colspan="9"><span></span></td>
				</tr>
				<tr>
					<th class="pkuportal-td-title" width="10%">教学大纲</th>
					<td colspan="9"><span>Online learning and optimization<br><br>Designing autonomous systems that can adapt to their environments is arguably one of the most important goals in computer science and engineering. In some cases the environment is too complex to be modeled, and the best is to take a robust approach: continuously optimize the system as it interacts with its environment. Online learning, a subfield of machine learning, provides the theoretical foundations to solve such problems. The course will provide an introduction to online learning, covering the basic techniques and ideas. We will also discuss its connections and applications to other areas of machine learning, as well as how the same techniques lead to efficient methods for large scale optimization.<br><br>Evaluation: final exam<br><br><br>Syllabus<br><br> <br>1. Introduction<br>2. A warmup example<br><br>Expert framework<br>3. Mistake bounds for the zero-one loss<br>4. Continous predictions and convex losses<br>5. Randomized exponential weights algorithm<br>6. Lower bounds for prediction with expert advice<br>7. Follow the perturbed leader<br>8-9. Large expert classes with structure<br>10. Boosting<br><br>Online convex optimization<br>11. Continuous exponential weights<br>12. Follow the regularized leader<br>13. Mirror descent<br>14. Lower bounds<br>15. Linear classification<br>16. Linear least squares<br>17-18. Fast rates and adaptivity<br>19. Connection to statistical learning theory<br>20-21. Learnability, from value to algorithms<br><br>Partial monitoring<br>22. Multi-armed bandits<br>23. Lower bounds<br>24. Linear bandits<br>25. Bandit convex optimization<br>26. Optimization for machine learning and<br>variance reduction methods<br>27. Stochastic multi-armed bandits<br>28. Best arm identification<br>29. Monte-Carlo tree search &amp; games
<br>课堂讲授，文献阅读
<br>课堂报告，考试

<br></span></td>
				</tr>
			</tbody></table>
			</td>
		</tr>
		
		
	</tbody></table>




</body></html>